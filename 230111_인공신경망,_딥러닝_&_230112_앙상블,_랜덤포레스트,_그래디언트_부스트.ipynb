{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PHdMTUuUa-ML",
        "0Z_OBBR_uFMc",
        "54d2SiiNhkNk",
        "GzbrhhaqoFag",
        "kVi4z_7RoD7_",
        "HpKi3vNT2DTQ",
        "wDgpjhx0ApEr"
      ],
      "mount_file_id": "1mspmby99BKqZHsamMLthLbC2Udc_xkwT",
      "authorship_tag": "ABX9TyPPK+kAEZZEKZSPoCOKoUcW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SohyunKimmm/SohyunKimmm/blob/main/230111_%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%2C_%EB%94%A5%EB%9F%AC%EB%8B%9D_%26_230112_%EC%95%99%EC%83%81%EB%B8%94%2C_%EB%9E%9C%EB%8D%A4%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8%2C_%EA%B7%B8%EB%9E%98%EB%94%94%EC%96%B8%ED%8A%B8_%EB%B6%80%EC%8A%A4%ED%8A%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#인공신경망\n"
      ],
      "metadata": {
        "id": "PHdMTUuUa-ML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1) 분류예측"
      ],
      "metadata": {
        "id": "i_D20pR2unJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngZO1WA9bRVS",
        "outputId": "d2e70fe7-8969-47a4-8a72-4e98ae381200"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Ashopping.csv\", encoding = \"cp949\")"
      ],
      "metadata": {
        "id": "Lm4gGd07bh0D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GpiIKTWcmE_",
        "outputId": "db775142-9900-400e-db75-b9cbe18240ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 21 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   고객ID       1000 non-null   int64  \n",
            " 1   이탈여부       1000 non-null   int64  \n",
            " 2   총매출액       1000 non-null   int64  \n",
            " 3   구매금액대      1000 non-null   int64  \n",
            " 4   방문빈도       1000 non-null   int64  \n",
            " 5   1회 평균매출액   1000 non-null   int64  \n",
            " 6   할인권 사용 횟수  1000 non-null   int64  \n",
            " 7   총 할인 금액    1000 non-null   int64  \n",
            " 8   고객등급       1000 non-null   int64  \n",
            " 9   구매유형       1000 non-null   int64  \n",
            " 10  클레임접수여부    1000 non-null   int64  \n",
            " 11  구매카테고리수    1000 non-null   int64  \n",
            " 12  거주지역       1000 non-null   int64  \n",
            " 13  성별         1000 non-null   int64  \n",
            " 14  고객 나이대     1000 non-null   int64  \n",
            " 15  거래기간       1000 non-null   int64  \n",
            " 16  할인민감여부     1000 non-null   int64  \n",
            " 17  Recency    1000 non-null   int64  \n",
            " 18  Frequency  1000 non-null   int64  \n",
            " 19  Monetary   1000 non-null   int64  \n",
            " 20  평균 구매주기    1000 non-null   float64\n",
            "dtypes: float64(1), int64(20)\n",
            "memory usage: 164.2 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIU8XUAacoTX",
        "outputId": "8983f4cf-b0a2-48ab-b8bd-80a5c4d4dddf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "고객ID         0\n",
              "이탈여부         0\n",
              "총매출액         0\n",
              "구매금액대        0\n",
              "방문빈도         0\n",
              "1회 평균매출액     0\n",
              "할인권 사용 횟수    0\n",
              "총 할인 금액      0\n",
              "고객등급         0\n",
              "구매유형         0\n",
              "클레임접수여부      0\n",
              "구매카테고리수      0\n",
              "거주지역         0\n",
              "성별           0\n",
              "고객 나이대       0\n",
              "거래기간         0\n",
              "할인민감여부       0\n",
              "Recency      0\n",
              "Frequency    0\n",
              "Monetary     0\n",
              "평균 구매주기      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "-voHAWHLcs9l",
        "outputId": "cb6f0e55-681b-4d12-d5af-3e5c96bf9e98"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              고객ID         이탈여부          총매출액        구매금액대        방문빈도  \\\n",
              "count  1000.000000  1000.000000  1.000000e+03  1000.000000  1000.00000   \n",
              "mean    500.500000     0.300000  5.858013e+06     0.700000    22.91100   \n",
              "std     288.819436     0.458487  5.812815e+06     0.781416    19.08217   \n",
              "min       1.000000     0.000000  1.886100e+06     0.000000     1.00000   \n",
              "25%     250.750000     0.000000  2.815905e+06     0.000000    10.75000   \n",
              "50%     500.500000     0.000000  4.092145e+06     0.500000    18.00000   \n",
              "75%     750.250000     1.000000  6.545392e+06     1.000000    28.00000   \n",
              "max    1000.000000     1.000000  6.759576e+07     2.000000   155.00000   \n",
              "\n",
              "           1회 평균매출액    할인권 사용 횟수        총 할인 금액         고객등급         구매유형  \\\n",
              "count  1.000000e+03  1000.000000    1000.000000  1000.000000  1000.000000   \n",
              "mean   3.521024e+05    16.027000  292371.670000     1.546000     2.656000   \n",
              "std    3.124636e+05     8.341334  111937.501042     0.498129     1.046307   \n",
              "min    2.708200e+04     1.000000    3750.000000     1.000000     1.000000   \n",
              "25%    1.631242e+05     9.000000  261686.250000     1.000000     2.000000   \n",
              "50%    2.582080e+05    17.000000  347500.000000     2.000000     2.000000   \n",
              "75%    4.268310e+05    23.000000  365400.000000     2.000000     4.000000   \n",
              "max    2.798500e+06    30.000000  400600.000000     2.000000     4.000000   \n",
              "\n",
              "       ...      구매카테고리수         거주지역           성별       고객 나이대         거래기간  \\\n",
              "count  ...  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
              "mean   ...     5.217000     5.147000     0.189000     3.964000  3495.891000   \n",
              "std    ...     2.224153     1.169084     0.391705     1.078827   965.966194   \n",
              "min    ...     1.000000     1.000000     0.000000     2.000000   827.000000   \n",
              "25%    ...     3.000000     4.000000     0.000000     3.000000  2871.000000   \n",
              "50%    ...     5.000000     5.000000     0.000000     4.000000  3836.000000   \n",
              "75%    ...     7.000000     6.000000     0.000000     5.000000  4207.250000   \n",
              "max    ...     9.000000     7.000000     1.000000     7.000000  5334.000000   \n",
              "\n",
              "            할인민감여부      Recency    Frequency     Monetary      평균 구매주기  \n",
              "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
              "mean      0.400000     4.925000     2.289000     4.129000   266.880824  \n",
              "std       0.490143     1.744253     1.669811     1.560383   254.077398  \n",
              "min       0.000000     1.000000     1.000000     1.000000    13.980645  \n",
              "25%       0.000000     4.000000     1.000000     3.000000   111.957671  \n",
              "50%       0.000000     5.000000     2.000000     4.000000   191.469697  \n",
              "75%       1.000000     7.000000     3.000000     6.000000   324.386218  \n",
              "max       1.000000     7.000000     7.000000     7.000000  1956.000000  \n",
              "\n",
              "[8 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c75e1f00-4157-47cf-bc4d-fac21324b602\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>고객ID</th>\n",
              "      <th>이탈여부</th>\n",
              "      <th>총매출액</th>\n",
              "      <th>구매금액대</th>\n",
              "      <th>방문빈도</th>\n",
              "      <th>1회 평균매출액</th>\n",
              "      <th>할인권 사용 횟수</th>\n",
              "      <th>총 할인 금액</th>\n",
              "      <th>고객등급</th>\n",
              "      <th>구매유형</th>\n",
              "      <th>...</th>\n",
              "      <th>구매카테고리수</th>\n",
              "      <th>거주지역</th>\n",
              "      <th>성별</th>\n",
              "      <th>고객 나이대</th>\n",
              "      <th>거래기간</th>\n",
              "      <th>할인민감여부</th>\n",
              "      <th>Recency</th>\n",
              "      <th>Frequency</th>\n",
              "      <th>Monetary</th>\n",
              "      <th>평균 구매주기</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1.000000e+03</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>1.000000e+03</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>500.500000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>5.858013e+06</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>22.91100</td>\n",
              "      <td>3.521024e+05</td>\n",
              "      <td>16.027000</td>\n",
              "      <td>292371.670000</td>\n",
              "      <td>1.546000</td>\n",
              "      <td>2.656000</td>\n",
              "      <td>...</td>\n",
              "      <td>5.217000</td>\n",
              "      <td>5.147000</td>\n",
              "      <td>0.189000</td>\n",
              "      <td>3.964000</td>\n",
              "      <td>3495.891000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>4.925000</td>\n",
              "      <td>2.289000</td>\n",
              "      <td>4.129000</td>\n",
              "      <td>266.880824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>288.819436</td>\n",
              "      <td>0.458487</td>\n",
              "      <td>5.812815e+06</td>\n",
              "      <td>0.781416</td>\n",
              "      <td>19.08217</td>\n",
              "      <td>3.124636e+05</td>\n",
              "      <td>8.341334</td>\n",
              "      <td>111937.501042</td>\n",
              "      <td>0.498129</td>\n",
              "      <td>1.046307</td>\n",
              "      <td>...</td>\n",
              "      <td>2.224153</td>\n",
              "      <td>1.169084</td>\n",
              "      <td>0.391705</td>\n",
              "      <td>1.078827</td>\n",
              "      <td>965.966194</td>\n",
              "      <td>0.490143</td>\n",
              "      <td>1.744253</td>\n",
              "      <td>1.669811</td>\n",
              "      <td>1.560383</td>\n",
              "      <td>254.077398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.886100e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>2.708200e+04</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3750.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>827.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.980645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>250.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.815905e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.75000</td>\n",
              "      <td>1.631242e+05</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>261686.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2871.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>111.957671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>500.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.092145e+06</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>18.00000</td>\n",
              "      <td>2.582080e+05</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>347500.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3836.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>191.469697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>750.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.545392e+06</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>28.00000</td>\n",
              "      <td>4.268310e+05</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>365400.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4207.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>324.386218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.759576e+07</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>155.00000</td>\n",
              "      <td>2.798500e+06</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>400600.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>5334.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>1956.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c75e1f00-4157-47cf-bc4d-fac21324b602')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c75e1f00-4157-47cf-bc4d-fac21324b602 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c75e1f00-4157-47cf-bc4d-fac21324b602');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "RrkJFuyBcHKE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXzjPnbfcwFz",
        "outputId": "e7bf134f-4262-4373-fdec-f6659078d411"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['고객ID', '이탈여부', '총매출액', '구매금액대', '방문빈도', '1회 평균매출액', '할인권 사용 횟수',\n",
              "       '총 할인 금액', '고객등급', '구매유형', '클레임접수여부', '구매카테고리수', '거주지역', '성별', '고객 나이대',\n",
              "       '거래기간', '할인민감여부', 'Recency', 'Frequency', 'Monetary', '평균 구매주기'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#분류예측\n",
        "\n",
        "#1. 변수선택\n",
        "X = df[[\"총매출액\", \"구매금액대\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\"]]\n",
        "Y = df[\"할인민감여부\"]\n",
        "\n",
        "#2. train-test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "#3. 데이터 전처리(preprocessing)\n",
        "ct = ColumnTransformer([(\"scaling\", StandardScaler(), [\"총매출액\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\"]), \n",
        "                        (\"onehot\", OneHotEncoder(sparse = False), [\"구매금액대\"])])\n",
        "ct.fit(X_train)\n",
        "\n",
        "X_train = ct.transform(X_train)\n",
        "X_test = ct.transform(X_test)\n",
        "\n",
        "#4. 오버샘플링\n",
        "smote = SMOTE(random_state = 0)\n",
        "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "6SCdHz6CczOj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. 모델링\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "nn_model = MLPClassifier(random_state = 0, alpha = 0.001, hidden_layer_sizes = [50])\n",
        "##5-1. 모형학습\n",
        "nn_model.fit(X_train, Y_train)\n",
        "\n",
        "#6. 예측\n",
        "Y_pred = nn_model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYeBOUSddI3A",
        "outputId": "d5c6b829-c4eb-4614-b0ab-d513d92b80bf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. 결과값 보고\n",
        "print(\"Y 예측값 \\n\", Y_pred)\n",
        "print(\"accuracy(train) : {:.3f}\".format(nn_model.score(X_train, Y_train)))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(Y_test, Y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-pKLH1ahH97",
        "outputId": "ee8185cb-1844-46b3-dfda-f689b6f2c0cd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y 예측값 \n",
            " [1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1\n",
            " 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1\n",
            " 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1\n",
            " 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0]\n",
            "accuracy(train) : 0.889\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.89      0.91       177\n",
            "           1       0.85      0.89      0.87       123\n",
            "\n",
            "    accuracy                           0.89       300\n",
            "   macro avg       0.89      0.89      0.89       300\n",
            "weighted avg       0.89      0.89      0.89       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2) 회귀예측"
      ],
      "metadata": {
        "id": "aAn9g0djoo-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSMWmM1hkL_c",
        "outputId": "5755ef43-c167-4dd1-9daf-d15152a23ef9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['고객ID', '이탈여부', '총매출액', '구매금액대', '방문빈도', '1회 평균매출액', '할인권 사용 횟수',\n",
              "       '총 할인 금액', '고객등급', '구매유형', '클레임접수여부', '구매카테고리수', '거주지역', '성별', '고객 나이대',\n",
              "       '거래기간', '할인민감여부', 'Recency', 'Frequency', 'Monetary', '평균 구매주기'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#1. 변수선택\n",
        "X = df[df.이탈여부 == 0][[\"방문빈도\", \"총 할인 금액\", \"고객등급\", \"구매유형\", \"거래기간\", \"할인민감여부\", \"평균 구매주기\"]]\n",
        "Y = df[df.이탈여부 == 0][\"1회 평균매출액\"]\n",
        "\n",
        "#2. train-test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "#3. 데이터 전처리(preprocessing)\n",
        "ct = ColumnTransformer([(\"scaling\", StandardScaler(), [\"방문빈도\", \"총 할인 금액\", \"거래기간\", \"평균 구매주기\"]), \n",
        "                        (\"onehot\", OneHotEncoder(sparse = False), [\"고객등급\", \"구매유형\", \"할인민감여부\"])])\n",
        "ct.fit(X_train)\n",
        "X_train = ct.transform(X_train)\n",
        "X_test = ct.transform(X_test)\n"
      ],
      "metadata": {
        "id": "oX5aLYnBotMs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. 모델링\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "#4-1. 모델생성\n",
        "nn_reg_model = MLPRegressor(random_state = 0, alpha = 1, max_iter = 1000, hidden_layer_sizes = [50,50]) #하이퍼 파라미터 값들\n",
        "\n",
        "#5. 모형학습\n",
        "nn_reg_model.fit(X_train, Y_train)\n",
        "Y_pred = nn_reg_model.predict(X_test)\n",
        "\n",
        "#6. 결과값 보고\n",
        "print(\"Y predict value : \\n\", Y_pred)\n",
        "print(\"train accuracy : {:.3f}\".format(nn_reg_model.score(X_train, Y_train)))\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "rmse = sqrt(mean_squared_error(Y_test, Y_pred))\n",
        "print(\"RMSE : {:.3f}\".format(rmse))\n",
        "\n",
        "#MLPRegressor참고사이트: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87Lhw8Zgp9N6",
        "outputId": "dc518cd3-2178-491f-c37c-53e1941d9949"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value : \n",
            " [367567.22799569 291986.7171292  286542.39507057 470299.18982825\n",
            " 199015.43022523 314985.16674704 120385.261673    99679.50744792\n",
            " 217207.85032309 321283.5694424  322281.24636508 139031.08090941\n",
            " 172479.30691299 307474.28058998 410674.7673535  282802.14200659\n",
            " 314022.43590656 344082.61534343 266080.18516834 299527.46258462\n",
            " 417918.96528793 213628.82138014 325703.16436337 276841.23973322\n",
            "  83289.89193415 370306.40166324 392255.59988171 148021.81268408\n",
            " 135257.54780785 288250.40948644 329672.79002503 409898.41100752\n",
            " 238134.15585775 368306.05176262 275290.22641391 239081.40629288\n",
            " 266321.69500888 375448.00373975 246605.1330584  641448.06611111\n",
            " 336904.01756403 215252.99856889 241609.33155848 271063.86770618\n",
            " 236907.37454933 382141.50786421 141280.68282403 289125.50608539\n",
            " 298377.15265302 346302.09403242 253397.19897089 508363.2921946\n",
            "  55430.52886374 316398.6845584  307992.35916116 291458.05176895\n",
            " 365830.87156821 337348.7566333  283210.3790915  258622.02741059\n",
            " 321376.87461064 327403.5219942  223207.07109362 339375.24930473\n",
            " 235633.20353306 207937.38459674 262238.13165728 155176.57248646\n",
            " 446908.94216759 158813.06040685 324047.59496836 252640.34325201\n",
            " 249188.03432076 255476.98860462 135204.18190746 486863.31488025\n",
            " 389836.25584169 417264.89960339 137349.62110706 330482.67927447\n",
            " 202882.37492681 244085.71728165 216194.80179372 178187.54506876\n",
            " 326307.74557151 334835.24310067 259056.39725013 318455.2747672\n",
            " 245978.7402989  349977.88045537 313576.00711254 360034.97890785\n",
            " 302180.47800664 346414.147532   237623.24093074 537825.48286654\n",
            " 306899.04560765 198016.87086714 356756.250312   126411.60509786\n",
            " 222199.09532434 293326.26886908 357988.60770913 274905.50147079\n",
            " 321045.37222521 308870.73491254 328625.82189502 200975.70277517\n",
            " 296192.52973971 279928.30765362 260548.00636601 243793.66083518\n",
            " 342931.62784114 486251.93814507 263093.75393189 222741.17780968\n",
            " 280110.46803518 314221.44235756 237481.55329132 271619.53638204\n",
            " 356806.58975663 367708.28247153 232120.08009347 279820.78777821\n",
            " 336140.0816819  301035.34267013 196964.58034718 242703.54496087\n",
            " 382656.27941644 264746.6531743   74865.40367927 229185.42533768\n",
            " 449451.21775471 333497.22792232 181490.12700764 308568.97923415\n",
            " 162126.67867317 273979.725792   312484.58403589 268972.60081153\n",
            " 294422.26870008 280549.89684759 369158.98151701 236824.03178162\n",
            " 191405.47832004 351443.85315144 298057.17180239 366323.17194185\n",
            " 160087.98542242 289201.47993591 187648.53842493 275254.96461994\n",
            " 296878.85455016 351165.94366769 685012.47921711 197831.35625512\n",
            "    817.42017899 336795.26572669 307319.33036176 286538.74627755\n",
            " 356601.98981613 413427.13794709 399347.35085261 181438.83136968\n",
            " 229242.10355842 330888.72112284 326911.57257259 213774.88283495\n",
            " 372421.41967877 243911.48617397   1134.30582295 251342.72805545\n",
            " 149121.8720192  518450.65557447 163722.93239689 330273.5210797\n",
            " 628305.94584286 229238.61156581 308547.9272714  150580.26882881\n",
            " 557029.72398692 782868.63100081 389620.35786206 424001.85956249\n",
            " 261503.41228481 201832.48696299 311830.67263387 308643.85612775\n",
            "  63366.06569213 343396.35371872 309710.08763841 315943.40002105\n",
            " 590729.84011205  81729.53183386 282024.21237733 289056.00731604\n",
            " 329795.69550524 107196.24435223 373365.59284941 119783.54874409\n",
            " 474729.87515126 108723.20464901 294099.46394641 263585.28216134\n",
            " 282962.56218265 223533.52354009 317057.78433315 328600.46139415\n",
            " 342669.79765667 293334.82175315]\n",
            "train accuracy : 0.300\n",
            "RMSE : 227578.429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#딥러닝"
      ],
      "metadata": {
        "id": "0Z_OBBR_uFMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###경고문 제거"
      ],
      "metadata": {
        "id": "1RiTqIJ9uHSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "cG29zpXwuLCd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DNN(Deep Learning: Deep Neural Network)"
      ],
      "metadata": {
        "id": "QToBKKc_uiBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1)분류예측"
      ],
      "metadata": {
        "id": "uTgNdHTkvKUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 변수선택\n",
        "X = df[[\"총매출액\", \"구매금액대\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\"]]\n",
        "Y = df[\"할인민감여부\"]\n",
        "\n",
        "#2. train-test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "#3. 데이터 전처리(preprocessing)\n",
        "ct = ColumnTransformer([(\"scaling\", StandardScaler(), [\"총매출액\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\"]), \n",
        "                        (\"onehot\", OneHotEncoder(sparse = False), [\"구매금액대\"])])\n",
        "ct.fit(X_train)\n",
        "\n",
        "X_train = ct.transform(X_train)\n",
        "X_test = ct.transform(X_test)\n",
        "\n",
        "#4. 오버샘플링\n",
        "smote = SMOTE(random_state = 0)\n",
        "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n"
      ],
      "metadata": {
        "id": "OwSk4dZVul4V"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#'keras' -> 딥러닝에서 사용하는 패키지\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential #'순차적'으로(back없이) 가는 모형 생성\n",
        "from keras.layers import Dense, Activation #'Dense': 노드의 수, 'Activation': 노드의 활성화 여부(값을 전달 할지 말지)\n",
        "from keras.metrics import Accuracy\n",
        "\n",
        "#5. 시드값 설정 : 시작점\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "#6. 모형생성\n",
        "model = keras.models.Sequential() #순차적으로 가는 모형 생성\n",
        "model.add(keras.layers.Dense(64, input_dim=7, activation = \"relu\")) \n",
        "#레이어를 add, 'input_dim = 7' -> 7차원의 신경망, activation함수의 디폴트: 'relu'함수\n",
        "model.add(keras.layers.Dense(64, activation = \"relu\"))\n",
        "model.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n",
        "\n",
        "#7. 모형 학습\n",
        "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"]) \n",
        "#'loss' -> accuracy에 반대되는 개념, 엔트로피? 불순도 확인(0과1에 대해서 분류)\n",
        "#'adam': 최적화시켜주는 함수\n",
        "\n",
        "history = model.fit(X_train, Y_train, validation_split = 0.2, epochs = 100, \n",
        "                    batch_size = 64, verbose = 2)\n",
        "#'epochs': 시도 횟수(1부터 100까지 계산을 시도) -> 높이게 되면 계산이 느려짐\n",
        "#'batch size' = Dense와 같은개념\n",
        "#'verbose'의 디폴트 값 = 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mBKUpz4uEL_",
        "outputId": "cb507bcf-78f9-48b5-9d3b-87cb3ce52682"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "11/11 - 1s - loss: 0.6829 - accuracy: 0.5888 - val_loss: 0.6856 - val_accuracy: 0.6529 - 1s/epoch - 98ms/step\n",
            "Epoch 2/100\n",
            "11/11 - 0s - loss: 0.5836 - accuracy: 0.8254 - val_loss: 0.6330 - val_accuracy: 0.6471 - 52ms/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "11/11 - 0s - loss: 0.5174 - accuracy: 0.8284 - val_loss: 0.6003 - val_accuracy: 0.6765 - 47ms/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "11/11 - 0s - loss: 0.4679 - accuracy: 0.8328 - val_loss: 0.5588 - val_accuracy: 0.7353 - 67ms/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "11/11 - 0s - loss: 0.4343 - accuracy: 0.8595 - val_loss: 0.5102 - val_accuracy: 0.8118 - 46ms/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "11/11 - 0s - loss: 0.4104 - accuracy: 0.8624 - val_loss: 0.5205 - val_accuracy: 0.7882 - 61ms/epoch - 6ms/step\n",
            "Epoch 7/100\n",
            "11/11 - 0s - loss: 0.3902 - accuracy: 0.8713 - val_loss: 0.4881 - val_accuracy: 0.8235 - 46ms/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "11/11 - 0s - loss: 0.3756 - accuracy: 0.8802 - val_loss: 0.4970 - val_accuracy: 0.8176 - 52ms/epoch - 5ms/step\n",
            "Epoch 9/100\n",
            "11/11 - 0s - loss: 0.3606 - accuracy: 0.8905 - val_loss: 0.4988 - val_accuracy: 0.8235 - 52ms/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "11/11 - 0s - loss: 0.3481 - accuracy: 0.8876 - val_loss: 0.4776 - val_accuracy: 0.8412 - 64ms/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "11/11 - 0s - loss: 0.3354 - accuracy: 0.8964 - val_loss: 0.5106 - val_accuracy: 0.8235 - 47ms/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "11/11 - 0s - loss: 0.3230 - accuracy: 0.8994 - val_loss: 0.4591 - val_accuracy: 0.8529 - 44ms/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "11/11 - 0s - loss: 0.3110 - accuracy: 0.9038 - val_loss: 0.4816 - val_accuracy: 0.8471 - 47ms/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "11/11 - 0s - loss: 0.3008 - accuracy: 0.9038 - val_loss: 0.4843 - val_accuracy: 0.8471 - 63ms/epoch - 6ms/step\n",
            "Epoch 15/100\n",
            "11/11 - 0s - loss: 0.2923 - accuracy: 0.9112 - val_loss: 0.4878 - val_accuracy: 0.8529 - 45ms/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "11/11 - 0s - loss: 0.2855 - accuracy: 0.9172 - val_loss: 0.4796 - val_accuracy: 0.8529 - 44ms/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "11/11 - 0s - loss: 0.2792 - accuracy: 0.9172 - val_loss: 0.4837 - val_accuracy: 0.8588 - 50ms/epoch - 5ms/step\n",
            "Epoch 18/100\n",
            "11/11 - 0s - loss: 0.2729 - accuracy: 0.9216 - val_loss: 0.4902 - val_accuracy: 0.8588 - 44ms/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "11/11 - 0s - loss: 0.2664 - accuracy: 0.9157 - val_loss: 0.4760 - val_accuracy: 0.8588 - 59ms/epoch - 5ms/step\n",
            "Epoch 20/100\n",
            "11/11 - 0s - loss: 0.2629 - accuracy: 0.9186 - val_loss: 0.4975 - val_accuracy: 0.8588 - 58ms/epoch - 5ms/step\n",
            "Epoch 21/100\n",
            "11/11 - 0s - loss: 0.2586 - accuracy: 0.9216 - val_loss: 0.4753 - val_accuracy: 0.8529 - 41ms/epoch - 4ms/step\n",
            "Epoch 22/100\n",
            "11/11 - 0s - loss: 0.2545 - accuracy: 0.9216 - val_loss: 0.4969 - val_accuracy: 0.8529 - 42ms/epoch - 4ms/step\n",
            "Epoch 23/100\n",
            "11/11 - 0s - loss: 0.2500 - accuracy: 0.9216 - val_loss: 0.4686 - val_accuracy: 0.8588 - 45ms/epoch - 4ms/step\n",
            "Epoch 24/100\n",
            "11/11 - 0s - loss: 0.2481 - accuracy: 0.9186 - val_loss: 0.4818 - val_accuracy: 0.8588 - 44ms/epoch - 4ms/step\n",
            "Epoch 25/100\n",
            "11/11 - 0s - loss: 0.2481 - accuracy: 0.9186 - val_loss: 0.4760 - val_accuracy: 0.8588 - 41ms/epoch - 4ms/step\n",
            "Epoch 26/100\n",
            "11/11 - 0s - loss: 0.2421 - accuracy: 0.9201 - val_loss: 0.4532 - val_accuracy: 0.8588 - 39ms/epoch - 4ms/step\n",
            "Epoch 27/100\n",
            "11/11 - 0s - loss: 0.2418 - accuracy: 0.9246 - val_loss: 0.4800 - val_accuracy: 0.8588 - 40ms/epoch - 4ms/step\n",
            "Epoch 28/100\n",
            "11/11 - 0s - loss: 0.2370 - accuracy: 0.9260 - val_loss: 0.4627 - val_accuracy: 0.8588 - 44ms/epoch - 4ms/step\n",
            "Epoch 29/100\n",
            "11/11 - 0s - loss: 0.2360 - accuracy: 0.9172 - val_loss: 0.4610 - val_accuracy: 0.8588 - 48ms/epoch - 4ms/step\n",
            "Epoch 30/100\n",
            "11/11 - 0s - loss: 0.2324 - accuracy: 0.9275 - val_loss: 0.4712 - val_accuracy: 0.8529 - 47ms/epoch - 4ms/step\n",
            "Epoch 31/100\n",
            "11/11 - 0s - loss: 0.2309 - accuracy: 0.9290 - val_loss: 0.4492 - val_accuracy: 0.8588 - 40ms/epoch - 4ms/step\n",
            "Epoch 32/100\n",
            "11/11 - 0s - loss: 0.2293 - accuracy: 0.9290 - val_loss: 0.4589 - val_accuracy: 0.8588 - 41ms/epoch - 4ms/step\n",
            "Epoch 33/100\n",
            "11/11 - 0s - loss: 0.2291 - accuracy: 0.9305 - val_loss: 0.4483 - val_accuracy: 0.8588 - 44ms/epoch - 4ms/step\n",
            "Epoch 34/100\n",
            "11/11 - 0s - loss: 0.2252 - accuracy: 0.9290 - val_loss: 0.4475 - val_accuracy: 0.8588 - 41ms/epoch - 4ms/step\n",
            "Epoch 35/100\n",
            "11/11 - 0s - loss: 0.2247 - accuracy: 0.9290 - val_loss: 0.4419 - val_accuracy: 0.8588 - 41ms/epoch - 4ms/step\n",
            "Epoch 36/100\n",
            "11/11 - 0s - loss: 0.2240 - accuracy: 0.9246 - val_loss: 0.4333 - val_accuracy: 0.8647 - 43ms/epoch - 4ms/step\n",
            "Epoch 37/100\n",
            "11/11 - 0s - loss: 0.2223 - accuracy: 0.9305 - val_loss: 0.4677 - val_accuracy: 0.8529 - 49ms/epoch - 4ms/step\n",
            "Epoch 38/100\n",
            "11/11 - 0s - loss: 0.2193 - accuracy: 0.9290 - val_loss: 0.4230 - val_accuracy: 0.8588 - 45ms/epoch - 4ms/step\n",
            "Epoch 39/100\n",
            "11/11 - 0s - loss: 0.2183 - accuracy: 0.9305 - val_loss: 0.4455 - val_accuracy: 0.8588 - 45ms/epoch - 4ms/step\n",
            "Epoch 40/100\n",
            "11/11 - 0s - loss: 0.2165 - accuracy: 0.9334 - val_loss: 0.4343 - val_accuracy: 0.8647 - 48ms/epoch - 4ms/step\n",
            "Epoch 41/100\n",
            "11/11 - 0s - loss: 0.2149 - accuracy: 0.9334 - val_loss: 0.4333 - val_accuracy: 0.8588 - 62ms/epoch - 6ms/step\n",
            "Epoch 42/100\n",
            "11/11 - 0s - loss: 0.2139 - accuracy: 0.9320 - val_loss: 0.4404 - val_accuracy: 0.8588 - 49ms/epoch - 4ms/step\n",
            "Epoch 43/100\n",
            "11/11 - 0s - loss: 0.2145 - accuracy: 0.9260 - val_loss: 0.4200 - val_accuracy: 0.8588 - 67ms/epoch - 6ms/step\n",
            "Epoch 44/100\n",
            "11/11 - 0s - loss: 0.2120 - accuracy: 0.9349 - val_loss: 0.4548 - val_accuracy: 0.8529 - 71ms/epoch - 6ms/step\n",
            "Epoch 45/100\n",
            "11/11 - 0s - loss: 0.2110 - accuracy: 0.9334 - val_loss: 0.4268 - val_accuracy: 0.8647 - 49ms/epoch - 4ms/step\n",
            "Epoch 46/100\n",
            "11/11 - 0s - loss: 0.2082 - accuracy: 0.9364 - val_loss: 0.4381 - val_accuracy: 0.8529 - 44ms/epoch - 4ms/step\n",
            "Epoch 47/100\n",
            "11/11 - 0s - loss: 0.2085 - accuracy: 0.9364 - val_loss: 0.4311 - val_accuracy: 0.8588 - 47ms/epoch - 4ms/step\n",
            "Epoch 48/100\n",
            "11/11 - 0s - loss: 0.2071 - accuracy: 0.9349 - val_loss: 0.4126 - val_accuracy: 0.8647 - 58ms/epoch - 5ms/step\n",
            "Epoch 49/100\n",
            "11/11 - 0s - loss: 0.2064 - accuracy: 0.9379 - val_loss: 0.4292 - val_accuracy: 0.8588 - 67ms/epoch - 6ms/step\n",
            "Epoch 50/100\n",
            "11/11 - 0s - loss: 0.2051 - accuracy: 0.9334 - val_loss: 0.4113 - val_accuracy: 0.8647 - 46ms/epoch - 4ms/step\n",
            "Epoch 51/100\n",
            "11/11 - 0s - loss: 0.2056 - accuracy: 0.9379 - val_loss: 0.4268 - val_accuracy: 0.8588 - 52ms/epoch - 5ms/step\n",
            "Epoch 52/100\n",
            "11/11 - 0s - loss: 0.2044 - accuracy: 0.9349 - val_loss: 0.4156 - val_accuracy: 0.8647 - 48ms/epoch - 4ms/step\n",
            "Epoch 53/100\n",
            "11/11 - 0s - loss: 0.2026 - accuracy: 0.9364 - val_loss: 0.4326 - val_accuracy: 0.8529 - 61ms/epoch - 6ms/step\n",
            "Epoch 54/100\n",
            "11/11 - 0s - loss: 0.2007 - accuracy: 0.9408 - val_loss: 0.4113 - val_accuracy: 0.8647 - 63ms/epoch - 6ms/step\n",
            "Epoch 55/100\n",
            "11/11 - 0s - loss: 0.1997 - accuracy: 0.9393 - val_loss: 0.4058 - val_accuracy: 0.8647 - 45ms/epoch - 4ms/step\n",
            "Epoch 56/100\n",
            "11/11 - 0s - loss: 0.1999 - accuracy: 0.9393 - val_loss: 0.4358 - val_accuracy: 0.8588 - 51ms/epoch - 5ms/step\n",
            "Epoch 57/100\n",
            "11/11 - 0s - loss: 0.1996 - accuracy: 0.9393 - val_loss: 0.3937 - val_accuracy: 0.8647 - 62ms/epoch - 6ms/step\n",
            "Epoch 58/100\n",
            "11/11 - 0s - loss: 0.1997 - accuracy: 0.9364 - val_loss: 0.4277 - val_accuracy: 0.8529 - 67ms/epoch - 6ms/step\n",
            "Epoch 59/100\n",
            "11/11 - 0s - loss: 0.1975 - accuracy: 0.9408 - val_loss: 0.3932 - val_accuracy: 0.8647 - 48ms/epoch - 4ms/step\n",
            "Epoch 60/100\n",
            "11/11 - 0s - loss: 0.1948 - accuracy: 0.9423 - val_loss: 0.4205 - val_accuracy: 0.8588 - 60ms/epoch - 5ms/step\n",
            "Epoch 61/100\n",
            "11/11 - 0s - loss: 0.1951 - accuracy: 0.9438 - val_loss: 0.4008 - val_accuracy: 0.8647 - 50ms/epoch - 5ms/step\n",
            "Epoch 62/100\n",
            "11/11 - 0s - loss: 0.1935 - accuracy: 0.9408 - val_loss: 0.4069 - val_accuracy: 0.8647 - 46ms/epoch - 4ms/step\n",
            "Epoch 63/100\n",
            "11/11 - 0s - loss: 0.1947 - accuracy: 0.9423 - val_loss: 0.4142 - val_accuracy: 0.8588 - 48ms/epoch - 4ms/step\n",
            "Epoch 64/100\n",
            "11/11 - 0s - loss: 0.1929 - accuracy: 0.9438 - val_loss: 0.4039 - val_accuracy: 0.8647 - 47ms/epoch - 4ms/step\n",
            "Epoch 65/100\n",
            "11/11 - 0s - loss: 0.1912 - accuracy: 0.9453 - val_loss: 0.4085 - val_accuracy: 0.8647 - 61ms/epoch - 6ms/step\n",
            "Epoch 66/100\n",
            "11/11 - 0s - loss: 0.1911 - accuracy: 0.9438 - val_loss: 0.4082 - val_accuracy: 0.8647 - 46ms/epoch - 4ms/step\n",
            "Epoch 67/100\n",
            "11/11 - 0s - loss: 0.1901 - accuracy: 0.9438 - val_loss: 0.4114 - val_accuracy: 0.8588 - 46ms/epoch - 4ms/step\n",
            "Epoch 68/100\n",
            "11/11 - 0s - loss: 0.1910 - accuracy: 0.9408 - val_loss: 0.3896 - val_accuracy: 0.8647 - 98ms/epoch - 9ms/step\n",
            "Epoch 69/100\n",
            "11/11 - 0s - loss: 0.1892 - accuracy: 0.9453 - val_loss: 0.3962 - val_accuracy: 0.8647 - 49ms/epoch - 4ms/step\n",
            "Epoch 70/100\n",
            "11/11 - 0s - loss: 0.1887 - accuracy: 0.9408 - val_loss: 0.4095 - val_accuracy: 0.8588 - 49ms/epoch - 4ms/step\n",
            "Epoch 71/100\n",
            "11/11 - 0s - loss: 0.1872 - accuracy: 0.9438 - val_loss: 0.3883 - val_accuracy: 0.8588 - 63ms/epoch - 6ms/step\n",
            "Epoch 72/100\n",
            "11/11 - 0s - loss: 0.1875 - accuracy: 0.9408 - val_loss: 0.3898 - val_accuracy: 0.8647 - 48ms/epoch - 4ms/step\n",
            "Epoch 73/100\n",
            "11/11 - 0s - loss: 0.1867 - accuracy: 0.9482 - val_loss: 0.4074 - val_accuracy: 0.8588 - 46ms/epoch - 4ms/step\n",
            "Epoch 74/100\n",
            "11/11 - 0s - loss: 0.1880 - accuracy: 0.9393 - val_loss: 0.3759 - val_accuracy: 0.8647 - 48ms/epoch - 4ms/step\n",
            "Epoch 75/100\n",
            "11/11 - 0s - loss: 0.1884 - accuracy: 0.9438 - val_loss: 0.4138 - val_accuracy: 0.8588 - 63ms/epoch - 6ms/step\n",
            "Epoch 76/100\n",
            "11/11 - 0s - loss: 0.1838 - accuracy: 0.9423 - val_loss: 0.3702 - val_accuracy: 0.8647 - 45ms/epoch - 4ms/step\n",
            "Epoch 77/100\n",
            "11/11 - 0s - loss: 0.1842 - accuracy: 0.9467 - val_loss: 0.3963 - val_accuracy: 0.8588 - 52ms/epoch - 5ms/step\n",
            "Epoch 78/100\n",
            "11/11 - 0s - loss: 0.1831 - accuracy: 0.9497 - val_loss: 0.3934 - val_accuracy: 0.8588 - 51ms/epoch - 5ms/step\n",
            "Epoch 79/100\n",
            "11/11 - 0s - loss: 0.1830 - accuracy: 0.9467 - val_loss: 0.3960 - val_accuracy: 0.8588 - 47ms/epoch - 4ms/step\n",
            "Epoch 80/100\n",
            "11/11 - 0s - loss: 0.1824 - accuracy: 0.9423 - val_loss: 0.3860 - val_accuracy: 0.8647 - 43ms/epoch - 4ms/step\n",
            "Epoch 81/100\n",
            "11/11 - 0s - loss: 0.1808 - accuracy: 0.9482 - val_loss: 0.3950 - val_accuracy: 0.8588 - 46ms/epoch - 4ms/step\n",
            "Epoch 82/100\n",
            "11/11 - 0s - loss: 0.1814 - accuracy: 0.9482 - val_loss: 0.3916 - val_accuracy: 0.8588 - 43ms/epoch - 4ms/step\n",
            "Epoch 83/100\n",
            "11/11 - 0s - loss: 0.1797 - accuracy: 0.9497 - val_loss: 0.3803 - val_accuracy: 0.8647 - 55ms/epoch - 5ms/step\n",
            "Epoch 84/100\n",
            "11/11 - 0s - loss: 0.1793 - accuracy: 0.9467 - val_loss: 0.3954 - val_accuracy: 0.8588 - 49ms/epoch - 4ms/step\n",
            "Epoch 85/100\n",
            "11/11 - 0s - loss: 0.1794 - accuracy: 0.9453 - val_loss: 0.3924 - val_accuracy: 0.8588 - 65ms/epoch - 6ms/step\n",
            "Epoch 86/100\n",
            "11/11 - 0s - loss: 0.1789 - accuracy: 0.9482 - val_loss: 0.3679 - val_accuracy: 0.8588 - 45ms/epoch - 4ms/step\n",
            "Epoch 87/100\n",
            "11/11 - 0s - loss: 0.1777 - accuracy: 0.9497 - val_loss: 0.3876 - val_accuracy: 0.8588 - 63ms/epoch - 6ms/step\n",
            "Epoch 88/100\n",
            "11/11 - 0s - loss: 0.1773 - accuracy: 0.9512 - val_loss: 0.3835 - val_accuracy: 0.8588 - 49ms/epoch - 4ms/step\n",
            "Epoch 89/100\n",
            "11/11 - 0s - loss: 0.1784 - accuracy: 0.9438 - val_loss: 0.3948 - val_accuracy: 0.8588 - 48ms/epoch - 4ms/step\n",
            "Epoch 90/100\n",
            "11/11 - 0s - loss: 0.1765 - accuracy: 0.9527 - val_loss: 0.3870 - val_accuracy: 0.8588 - 47ms/epoch - 4ms/step\n",
            "Epoch 91/100\n",
            "11/11 - 0s - loss: 0.1758 - accuracy: 0.9497 - val_loss: 0.3666 - val_accuracy: 0.8647 - 48ms/epoch - 4ms/step\n",
            "Epoch 92/100\n",
            "11/11 - 0s - loss: 0.1772 - accuracy: 0.9423 - val_loss: 0.3699 - val_accuracy: 0.8588 - 45ms/epoch - 4ms/step\n",
            "Epoch 93/100\n",
            "11/11 - 0s - loss: 0.1755 - accuracy: 0.9512 - val_loss: 0.3941 - val_accuracy: 0.8588 - 44ms/epoch - 4ms/step\n",
            "Epoch 94/100\n",
            "11/11 - 0s - loss: 0.1753 - accuracy: 0.9467 - val_loss: 0.3659 - val_accuracy: 0.8588 - 49ms/epoch - 4ms/step\n",
            "Epoch 95/100\n",
            "11/11 - 0s - loss: 0.1743 - accuracy: 0.9512 - val_loss: 0.3836 - val_accuracy: 0.8588 - 52ms/epoch - 5ms/step\n",
            "Epoch 96/100\n",
            "11/11 - 0s - loss: 0.1743 - accuracy: 0.9482 - val_loss: 0.3720 - val_accuracy: 0.8588 - 50ms/epoch - 5ms/step\n",
            "Epoch 97/100\n",
            "11/11 - 0s - loss: 0.1739 - accuracy: 0.9527 - val_loss: 0.3804 - val_accuracy: 0.8588 - 75ms/epoch - 7ms/step\n",
            "Epoch 98/100\n",
            "11/11 - 0s - loss: 0.1749 - accuracy: 0.9423 - val_loss: 0.3558 - val_accuracy: 0.8647 - 61ms/epoch - 6ms/step\n",
            "Epoch 99/100\n",
            "11/11 - 0s - loss: 0.1722 - accuracy: 0.9482 - val_loss: 0.3841 - val_accuracy: 0.8588 - 46ms/epoch - 4ms/step\n",
            "Epoch 100/100\n",
            "11/11 - 0s - loss: 0.1716 - accuracy: 0.9497 - val_loss: 0.3719 - val_accuracy: 0.8588 - 44ms/epoch - 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#epoch에서 나온 loss, accuracy를 찾아서 크로스가 되는 부분 찾아보기\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "#loss(오차) 그리기\n",
        "loss_ax.plot(history.history[\"loss\"], \"y\", label = \"train loss\")  #history에서 loss를 찾아서 yellow색상으로, 라벨은 \"train loss\"\n",
        "loss_ax.plot(history.history[\"val_loss\"], \"r\", label = \"val loss\")\n",
        "loss_ax.set_xlabel(\"epoch\")\n",
        "loss_ax.set_ylabel(\"loss\")\n",
        "acc_ax.legend(loc = \"lower right\")  #legend: 범례\n",
        "\n",
        "\n",
        "#accuracy(정확도) 그리기\n",
        "acc_ax.plot(history.history[\"accuracy\"], \"b\", label = \"train acc\")\n",
        "acc_ax.plot(history.history[\"val_accuracy\"], \"g\", label = \"val acc\")\n",
        "acc_ax.set_ylabel(\"accuracy\")\n",
        "acc_ax.legend(loc = \"upper right\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#'적정수준'에서 끝내겠다 -> epoch(x축)을 20정도로 잡으면 되지 않을까. (20정도로 잡으면 과적합은 아니다.)"
      ],
      "metadata": {
        "id": "u-znFHcM4Toz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "afd70b4a-902c-40f6-a5c8-88969ea7b3ae"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#epoch값 20으로 넣어보기\n",
        "model.fit(X_train, Y_train, validation_split = 0.2, epochs = 20, \n",
        "          batch_size = 64, verbose = 1)"
      ],
      "metadata": {
        "id": "XhYlpiw-7hR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461a2617-2150-49e1-f62a-555fb358ecab"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.1702 - accuracy: 0.9497 - val_loss: 0.3680 - val_accuracy: 0.8588\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1697 - accuracy: 0.9512 - val_loss: 0.3609 - val_accuracy: 0.8588\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1706 - accuracy: 0.9497 - val_loss: 0.3671 - val_accuracy: 0.8588\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1696 - accuracy: 0.9497 - val_loss: 0.3579 - val_accuracy: 0.8647\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1683 - accuracy: 0.9512 - val_loss: 0.3734 - val_accuracy: 0.8588\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1685 - accuracy: 0.9512 - val_loss: 0.3627 - val_accuracy: 0.8588\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1688 - accuracy: 0.9527 - val_loss: 0.3614 - val_accuracy: 0.8588\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1687 - accuracy: 0.9527 - val_loss: 0.3717 - val_accuracy: 0.8588\n",
            "Epoch 9/20\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1661 - accuracy: 0.9512 - val_loss: 0.3677 - val_accuracy: 0.8588\n",
            "Epoch 10/20\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1667 - accuracy: 0.9512 - val_loss: 0.3754 - val_accuracy: 0.8588\n",
            "Epoch 11/20\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1667 - accuracy: 0.9512 - val_loss: 0.3679 - val_accuracy: 0.8588\n",
            "Epoch 12/20\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1658 - accuracy: 0.9527 - val_loss: 0.3560 - val_accuracy: 0.8588\n",
            "Epoch 13/20\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1645 - accuracy: 0.9512 - val_loss: 0.3706 - val_accuracy: 0.8588\n",
            "Epoch 14/20\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1645 - accuracy: 0.9512 - val_loss: 0.3531 - val_accuracy: 0.8588\n",
            "Epoch 15/20\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1635 - accuracy: 0.9512 - val_loss: 0.3723 - val_accuracy: 0.8588\n",
            "Epoch 16/20\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1644 - accuracy: 0.9527 - val_loss: 0.3496 - val_accuracy: 0.8588\n",
            "Epoch 17/20\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1661 - accuracy: 0.9497 - val_loss: 0.3620 - val_accuracy: 0.8588\n",
            "Epoch 18/20\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1653 - accuracy: 0.9497 - val_loss: 0.3627 - val_accuracy: 0.8588\n",
            "Epoch 19/20\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1621 - accuracy: 0.9527 - val_loss: 0.3453 - val_accuracy: 0.8647\n",
            "Epoch 20/20\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1609 - accuracy: 0.9527 - val_loss: 0.3684 - val_accuracy: 0.8588\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f18d37d3ca0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = model.predict(X_test, verbose = 0)\n",
        "Y_class = np.round(Y_pred, 0)\n",
        "train_score = model.evaluate(X_train, Y_train, verbose = 0)\n",
        "test_score = model.evaluate(X_test, Y_test, verbose = 0)\n",
        "print(\"Y 예측값 : \\n\", Y_pred[:5])\n",
        "print(\"Y 예측 클래스 : \\n\", Y_class[:5])\n",
        "print(\"train accuracy : {:.3f}\". format(train_score[0], train_score[1]))\n",
        "print(\"test accuracy : {:.3f}\".format(test_score[0], train_score[1]))"
      ],
      "metadata": {
        "id": "3ZuJwRPI73vQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16bfb3e7-78de-46dc-9c32-4bddc008c9b6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y 예측값 : \n",
            " [[0.93126047]\n",
            " [1.        ]\n",
            " [0.01772536]\n",
            " [0.01244921]\n",
            " [0.99973166]]\n",
            "Y 예측 클래스 : \n",
            " [[1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]]\n",
            "train accuracy : 0.202\n",
            "test accuracy : 0.224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "230112\n",
        "####2) 회귀예측"
      ],
      "metadata": {
        "id": "LhcufWL3TXKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "bgsgs5eJVbLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13788f4-f16b-4d42-ea7c-c494e85b2751"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 21 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   고객ID       1000 non-null   int64  \n",
            " 1   이탈여부       1000 non-null   int64  \n",
            " 2   총매출액       1000 non-null   int64  \n",
            " 3   구매금액대      1000 non-null   int64  \n",
            " 4   방문빈도       1000 non-null   int64  \n",
            " 5   1회 평균매출액   1000 non-null   int64  \n",
            " 6   할인권 사용 횟수  1000 non-null   int64  \n",
            " 7   총 할인 금액    1000 non-null   int64  \n",
            " 8   고객등급       1000 non-null   int64  \n",
            " 9   구매유형       1000 non-null   int64  \n",
            " 10  클레임접수여부    1000 non-null   int64  \n",
            " 11  구매카테고리수    1000 non-null   int64  \n",
            " 12  거주지역       1000 non-null   int64  \n",
            " 13  성별         1000 non-null   int64  \n",
            " 14  고객 나이대     1000 non-null   int64  \n",
            " 15  거래기간       1000 non-null   int64  \n",
            " 16  할인민감여부     1000 non-null   int64  \n",
            " 17  Recency    1000 non-null   int64  \n",
            " 18  Frequency  1000 non-null   int64  \n",
            " 19  Monetary   1000 non-null   int64  \n",
            " 20  평균 구매주기    1000 non-null   float64\n",
            "dtypes: float64(1), int64(20)\n",
            "memory usage: 164.2 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "1dNP-ultVdsQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f27ee71a-ef05-4f2b-e9d3-b7266b3ea832"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['고객ID', '이탈여부', '총매출액', '구매금액대', '방문빈도', '1회 평균매출액', '할인권 사용 횟수',\n",
              "       '총 할인 금액', '고객등급', '구매유형', '클레임접수여부', '구매카테고리수', '거주지역', '성별', '고객 나이대',\n",
              "       '거래기간', '할인민감여부', 'Recency', 'Frequency', 'Monetary', '평균 구매주기'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 데이터 만들기\n",
        "X = df[df.이탈여부 == 0][[\"방문빈도\", \"총 할인 금액\", \"구매카테고리수\", \"거래기간\"]]\n",
        "Y = np.log1p(df[df.이탈여부 == 0][\"1회 평균매출액\"])"
      ],
      "metadata": {
        "id": "3xdlm2K0TdN_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. train-test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "#3. 데이터 전처리(preprocessing)\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#4. seed값 설정\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "#5. 모형생성\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(64, input_dim=4, activation = \"relu\")) \n",
        "model.add(keras.layers.Dense(64, activation = \"relu\"))\n",
        "model.add(keras.layers.Dense(64, activation = \"relu\"))\n",
        "\n",
        "#6. 모형 학습\n",
        "model.compile(loss = \"mse\", optimizer = \"SGD\")\n",
        "#'optimizer': 작은 오차를 가지는 지점을 찾아주는 장치. 'SGD': 확률적 경사 하강법\n",
        "Y_pred = np.round(model.predict(X_test[:5], verbose = 0), 3)\n",
        "print(\"Y predict value \\n\", Y_pred)\n",
        "\n",
        "#7. 모형 평가\n",
        "train_score = model.evaluate(X_train, Y_train, verbose = 0)\n",
        "test_score = model.evaluate(X_test, Y_test, verbose = 0)\n",
        "print(\"train mse : {:.3f}\".format(train_score))\n",
        "print(\"test mse : {:.3f}\".format(test_score))\n"
      ],
      "metadata": {
        "id": "uS_FGZxwV2iq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "426122ee-b7f3-4989-e3e9-ce44c2614a30"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value \n",
            " [[0.    0.    0.    0.234 0.    0.163 0.052 0.    0.    0.    0.    0.32\n",
            "  0.013 0.    0.218 0.009 0.111 0.    0.182 0.    0.    0.    0.474 0.143\n",
            "  0.    0.    0.    0.    0.    0.    0.325 0.    0.    0.224 0.    0.098\n",
            "  0.115 0.    0.    0.055 0.    0.    0.    0.    0.    0.    0.    0.\n",
            "  0.    0.322 0.12  0.098 0.    0.336 0.142 0.    0.    0.394 0.    0.498\n",
            "  0.    0.    0.182 0.   ]\n",
            " [0.    0.    0.    0.002 0.    0.134 0.114 0.    0.109 0.    0.    0.058\n",
            "  0.061 0.043 0.042 0.027 0.06  0.097 0.114 0.    0.    0.001 0.14  0.\n",
            "  0.081 0.017 0.    0.034 0.    0.    0.012 0.009 0.    0.045 0.    0.039\n",
            "  0.    0.    0.    0.109 0.    0.009 0.004 0.    0.    0.    0.    0.\n",
            "  0.059 0.097 0.074 0.026 0.    0.014 0.184 0.    0.002 0.061 0.075 0.146\n",
            "  0.    0.    0.289 0.   ]\n",
            " [0.    0.    0.079 0.012 0.    0.056 0.061 0.    0.175 0.    0.089 0.14\n",
            "  0.002 0.    0.254 0.04  0.094 0.183 0.181 0.    0.058 0.    0.385 0.058\n",
            "  0.    0.    0.    0.057 0.    0.    0.144 0.051 0.07  0.104 0.023 0.032\n",
            "  0.036 0.    0.    0.162 0.    0.    0.    0.    0.    0.    0.    0.\n",
            "  0.054 0.124 0.055 0.014 0.    0.081 0.055 0.    0.057 0.15  0.048 0.408\n",
            "  0.    0.    0.363 0.   ]\n",
            " [0.    0.002 0.    0.121 0.    0.06  0.097 0.115 0.    0.    0.232 0.095\n",
            "  0.    0.    0.11  0.    0.147 0.153 0.168 0.    0.    0.    0.268 0.204\n",
            "  0.    0.    0.    0.    0.    0.    0.448 0.    0.    0.254 0.    0.075\n",
            "  0.214 0.    0.    0.004 0.043 0.    0.024 0.    0.    0.    0.036 0.\n",
            "  0.    0.219 0.    0.2   0.    0.357 0.169 0.    0.134 0.232 0.016 0.495\n",
            "  0.    0.    0.222 0.   ]\n",
            " [0.    0.    0.    0.024 0.018 0.053 0.087 0.088 0.054 0.    0.102 0.003\n",
            "  0.    0.    0.068 0.    0.    0.145 0.005 0.    0.    0.    0.111 0.075\n",
            "  0.    0.096 0.    0.    0.    0.    0.013 0.    0.    0.064 0.    0.094\n",
            "  0.166 0.    0.    0.    0.    0.    0.    0.    0.014 0.    0.06  0.\n",
            "  0.    0.132 0.    0.171 0.    0.028 0.094 0.    0.    0.    0.    0.068\n",
            "  0.    0.    0.053 0.   ]]\n",
            "train mse : 152.833\n",
            "test mse : 152.401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "w1eE3AJXhbfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#앙상블\n",
        "- Begging: 모형을 여러가지 뽑고 평균해서 적절하게 값을 뽑아줌, 병렬구조를 가짐\n",
        "  (ex_random forest)\n",
        "- Boosting: 모형을 하나 뽑고 잘못한걸 학습해서 다음 모델로 넘긴다. weight를 계속해서 새롭게 부여함. 순차적인 직렬구조를 가짐, 맨 마지막에 좋은 모형이 나옴, 오차에 대한 학습이 잘 안되면 개선에 대한 한계가 있다. \n",
        "  (ex_ Gradiant Boost)\n",
        "- 앙상블모형 주의점: '과적합'\n",
        "- Boosting에 비해서는 Begging이 과적합 방지에는 더 나음, 하지만 둘 다 과적합 위험성 높음"
      ],
      "metadata": {
        "id": "54d2SiiNhkNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1) 분류예측"
      ],
      "metadata": {
        "id": "GzbrhhaqoFag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#분류예측\n",
        "\n",
        "#1. 변수선택\n",
        "X = df[df.이탈여부 == 0][[\"총매출액\", \"구매금액대\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\"]]\n",
        "Y = df[df.이탈여부 == 0][\"할인민감여부\"]\n",
        "\n",
        "#2. train-test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "#3. 데이터 전처리(preprocessing)\n",
        "ct = ColumnTransformer([(\"scaling\", StandardScaler(), [\"총매출액\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\"]), \n",
        "                        (\"onehot\", OneHotEncoder(sparse = False), [\"구매금액대\"])])\n",
        "ct.fit(X_train)\n",
        "X_train = ct.transform(X_train)\n",
        "X_test = ct.transform(X_test)\n",
        "\n",
        "#4. 오버샘플링\n",
        "smote = SMOTE(random_state = 0)\n",
        "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n"
      ],
      "metadata": {
        "id": "Uv81_0oEgxaX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. 단일모형 생성\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "dtree = DecisionTreeClassifier(random_state = 0)\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "#6. 앙상블 모형 생성\n",
        "#보팅(voting): 여러가지 분류기를 만들어 놓고 그중에 가장 좋은 모형의 알고리즘을 가지고와서 분석하는 것\n",
        "#knn과 Decisiontree모델 두가지를 넣었음\n",
        "#'soft' voting: 어떤 모형에 너무 치우치지 않고 알고리즘 중에 가장 적절한것을 가지고 와서 과적합적인 부분들을 해결할 수 있게..\n",
        "#soft보팅이 hard보팅보다 성능이 더 좋음\n",
        "model = VotingClassifier(estimators = [(\"K-NN\", knn), (\"Dtree\", dtree)], voting = \"soft\")\n",
        "\n",
        "#7. 모형학습\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "#8. 예측 및 모형 성능평가\n",
        "Y_pred = model.predict(X_test)\n",
        "print(\"Y predict value : \\n\", Y_pred)\n",
        "\n",
        "#8-1. 보팅모형 정확도\n",
        "print(\"voting classifier accuracy : {:0.3f}\".format(model.score(X_test, Y_test)))\n",
        "\n",
        "#8-2. 개별모형 정확도\n",
        "classifiers = [dtree, knn]\n",
        "for classifier in classifiers:\n",
        "    classifier.fit(X_train, Y_train)\n",
        "    class_name = classifier.__class__.__name__\n",
        "    print(\"{0} accuracy : {1:.3f}\".format(class_name, classifier.score(X_test, Y_test)))\n",
        "\n",
        "#여기서는 voing모형 정확도가 가장 높음! (항상 그런것은 x)\n",
        "#'가장 좋은것' 이라는 '기준'이 없기 때문에 항상 비교해야함 ->  가장 적절한 모형을 찾아는 것이 분석가가 해야할 일"
      ],
      "metadata": {
        "id": "LtMD5FfFhxlA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de9e0ff1-4a26-4f36-f2dc-7704636bf048"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value : \n",
            " [1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0\n",
            " 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0]\n",
            "voting classifier accuracy : 0.924\n",
            "DecisionTreeClassifier accuracy : 0.914\n",
            "KNeighborsClassifier accuracy : 0.819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2)회귀예측"
      ],
      "metadata": {
        "id": "kVi4z_7RoD7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#1. 변수선택\n",
        "X = df[df.이탈여부 == 0][[\"방문빈도\", \"총 할인 금액\", \"고객등급\", \"구매유형\", \"거래기간\", \"할인민감여부\", \"평균 구매주기\"]]\n",
        "Y = np.log1p(df[df.이탈여부 == 0][\"1회 평균매출액\"])\n",
        "\n",
        "#2. train-test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "#3. 데이터 전처리(preprocessing)\n",
        "ct = ColumnTransformer([(\"scaling\", StandardScaler(), [\"방문빈도\", \"총 할인 금액\", \"거래기간\", \"평균 구매주기\"]), \n",
        "                        (\"onehot\", OneHotEncoder(sparse = False), [\"고객등급\", \"구매유형\", \"할인민감여부\"])])\n",
        "ct.fit(X_train)\n",
        "X_train = ct.transform(X_train)\n",
        "X_test = ct.transform(X_test)"
      ],
      "metadata": {
        "id": "jb-V_YEioDRy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. 단일 모형 생성\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "svr = SVR()\n",
        "mlp = MLPRegressor(random_state=0)\n",
        "\n",
        "#5. voting model 생성\n",
        "model = VotingRegressor(estimators = [(\"SVR\", svr), (\"MLP\", mlp)])\n",
        "\n",
        "#6. 모형학습\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "#7. 예측 및 평가\n",
        "Y_pred = model.predict(X_test)\n",
        "print(\"Y predict value: \\n\", Y_pred)\n",
        "\n",
        "#8-1. 예측 보팅모형 정확도\n",
        "print(\"voting regressor accuracy(R2): {0:.3f}\".format(model.score(X_test, Y_test)))\n",
        "\n",
        "#8-2. 개별모형 정확도\n",
        "Regressors = [svr, mlp]\n",
        "for Regressor in Regressors:\n",
        "    Regressor.fit(X_train,Y_train)\n",
        "    class_name = Regressor.__class__.__name__\n",
        "    print(\"{0} accuracy : {1:.3f}\".format(class_name, Regressor.score(X_test, Y_test)))"
      ],
      "metadata": {
        "id": "pTvAb1pboWk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "493c61a5-235a-4789-c60c-9b8a3e0b160b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value: \n",
            " [12.72637282 12.15785517 12.09686124 13.26831502 11.60231958 12.47233857\n",
            " 11.00403584 12.11236139 11.92418038 12.36295705 12.33471768 11.93634366\n",
            " 12.10248328 12.17485959 12.54047237 12.21174042 12.79796392 12.85868158\n",
            " 12.14723369 12.51820132 12.73655925 11.51053651 12.16572772 12.61967869\n",
            " 10.97450508 13.11252037 12.5792913  12.13627155 12.04792775 11.98604929\n",
            " 12.79524713 12.95793432 12.51574594 13.14826771 12.21608558 11.91077705\n",
            " 11.84798695 12.99567742 11.95362493 13.66455063 12.85068225 12.1427706\n",
            " 12.49680643 12.12413092 12.41795186 12.43119996 11.29956398 12.14660018\n",
            " 12.69704459 12.43998557 12.01109351 13.29011823 11.1046136  12.92296162\n",
            " 12.8522466  12.08692287 12.46809297 12.34155171 12.26251834 12.06954504\n",
            " 12.87142564 12.96736873 11.66603809 12.35227513 12.06159606 12.155967\n",
            " 12.10647664 11.48482286 12.69218789 11.79509489 12.20279207 11.67227129\n",
            " 11.93748831 11.75242257 12.08799173 12.79011812 12.43771399 12.93671652\n",
            " 12.21409548 12.39436336 12.71746922 11.95532413 11.82312984 12.16287274\n",
            " 12.28686047 12.69164266 12.44251859 12.77121709 11.83604445 12.83491032\n",
            " 12.14491251 13.21743021 12.50243799 13.06892645 11.92415719 13.03858984\n",
            " 12.88053564 12.160099   12.91472933 11.63221781 11.94370304 12.44004892\n",
            " 12.87464916 11.8729167  12.3654078  12.55856624 12.84071252 11.71225848\n",
            " 12.26380165 12.45982361 12.02043293 11.9421507  12.73253879 13.02635162\n",
            " 12.04973161 12.33438799 12.41827333 12.86241083 12.40875392 12.5968783\n",
            " 12.81779588 12.8274637  12.19010157 12.59904112 12.84774747 12.18532332\n",
            " 12.22778031 11.8054232  13.12475682 11.95865789 11.81489928 11.55897355\n",
            " 13.07522943 12.28451019 11.8789114  12.15561689 11.51884822 12.18398225\n",
            " 12.69227703 11.85075753 12.26168017 12.16159193 12.65547847 11.98416886\n",
            " 12.1825295  12.53395352 12.63117748 12.89234566 12.03663241 12.06490339\n",
            " 12.17004133 12.65506435 12.64528232 12.859696   13.98345371 11.56676754\n",
            " 12.14814582 12.76169623 12.08281248 12.78057339 12.92206256 12.64509877\n",
            " 12.47824953 11.8252805  12.39300142 12.55519649 12.769953   11.36679481\n",
            " 12.96480349 12.50216886 11.50980876 11.95884547 11.44958371 13.16010026\n",
            " 12.05559067 12.71797611 13.99850887 11.871165   12.31221069 11.61007956\n",
            " 13.80229617 14.53772869 12.52183126 12.69249083 12.21280298 11.72885039\n",
            " 12.22192476 12.20813083 11.39735772 12.84417613 12.23391116 12.77976915\n",
            " 13.54789146 11.70010427 12.13965267 12.01776146 12.34314015 12.29642101\n",
            " 12.7601278  11.25118371 13.264752   11.77360757 12.25216632 11.80519914\n",
            " 12.70641052 11.83457641 12.32938499 12.20461754 12.97324333 12.52229619]\n",
            "voting regressor accuracy(R2): 0.538\n",
            "SVR accuracy : 0.608\n",
            "MLPRegressor accuracy : 0.225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest"
      ],
      "metadata": {
        "id": "HpKi3vNT2DTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1)분류예측"
      ],
      "metadata": {
        "id": "nF7CnVBo2LNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 분류선택\n",
        "X = df[[\"총매출액\", \"구매금액대\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\"]]\n",
        "Y = df[\"할인민감여부\"]\n",
        "\n",
        "#2. train-test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#3. 데이터 전처리(preprocessing)\n",
        "ct = ColumnTransformer([(\"scaling\", StandardScaler(), [\"총매출액\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\"]), \n",
        "                        (\"onehot\", OneHotEncoder(sparse=False), [\"구매금액대\"])])\n",
        "ct.fit(X_train)\n",
        "X_train = ct.transform(X_train)\n",
        "X_test = ct.transform(X_test)\n",
        "\n",
        "#4. 오버샘플링\n",
        "smote = SMOTE(random_state = 0)\n",
        "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n"
      ],
      "metadata": {
        "id": "bEQy3XRW2MmX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. 모형생성\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(random_state=0, n_estimators=300, max_depth=3) #max_depth=3인 decision tree모형을 300개 만들\n",
        "model.fit(X_train, Y_train)\n",
        "Y_pred = model.predict(X_test)\n",
        "print(\"Y predict value : \\n\", Y_pred)\n",
        "print(\"accuracy(test) : {:.3f}\".format(model.score(X_test, Y_test)))\n",
        "print(classification_report(Y_test, Y_pred))\n",
        "\n",
        "#랜덤포레스트는 과적합 될 확률이 굉장히 높음\n",
        "#파라미터를 조정해가면서 과적합 되지 않도록 해야함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbGGA43-3D0Q",
        "outputId": "3aef34a2-9e12-4f73-94fa-88113ffc26d4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value : \n",
            " [1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1\n",
            " 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0]\n",
            "accuracy(test) : 0.947\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.99      0.96       177\n",
            "           1       0.99      0.88      0.93       123\n",
            "\n",
            "    accuracy                           0.95       300\n",
            "   macro avg       0.96      0.94      0.94       300\n",
            "weighted avg       0.95      0.95      0.95       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2)회귀예측"
      ],
      "metadata": {
        "id": "N-NKzr_r6KDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#1. 변수선택\n",
        "X = df[df.이탈여부 == 0][[\"방문빈도\", \"총 할인 금액\", \"고객등급\", \"구매유형\", \"거래기간\", \"할인민감여부\", \"평균 구매주기\"]]\n",
        "Y = np.log1p(df[df.이탈여부 == 0][\"1회 평균매출액\"])\n",
        "\n",
        "#2. train-test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "#3. 데이터 전처리(preprocessing)\n",
        "ct = ColumnTransformer([(\"scaling\", StandardScaler(), [\"방문빈도\", \"총 할인 금액\", \"거래기간\", \"평균 구매주기\"]), \n",
        "                        (\"onehot\", OneHotEncoder(sparse = False), [\"고객등급\", \"구매유형\", \"할인민감여부\"])])\n",
        "ct.fit(X_train)\n",
        "X_train = ct.transform(X_train)\n",
        "X_test = ct.transform(X_test)"
      ],
      "metadata": {
        "id": "-XKnu-WS42Pn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. 모형생성\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "model = RandomForestRegressor(random_state = 0, n_estimators = 100, max_depth = 4)\n",
        "\n",
        "#5. 모형 예측 평가\n",
        "model.fit(X_train, Y_train)\n",
        "Y_pred = model.predict(X_test)\n",
        "print(\"Y predict value : \\n\", Y_pred)\n",
        "print(\"accuracy(R2) : {:.3f}\".format(model.score(X_train, Y_train)))\n",
        "rmse = sqrt(mean_squared_error(Y_test, Y_pred))\n",
        "print(\"RMSE: \", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjbpJhaZ6eDe",
        "outputId": "784e8f6b-d68c-43fb-cc86-54b5656d4372"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value : \n",
            " [12.68641835 12.12514749 12.04819279 13.04580899 11.50906768 12.45183582\n",
            " 11.28296585 12.28747852 11.86494273 12.35358139 12.3872265  12.00638164\n",
            " 12.3207311  12.16711163 12.72553    12.43847827 12.70631401 12.74484282\n",
            " 11.93548044 12.48363036 12.70833215 11.50205816 12.3872265  12.61313248\n",
            " 11.14155529 12.85315034 12.69646319 12.2799654  12.29330952 12.07066037\n",
            " 12.7318708  13.03072124 12.44587063 12.83922589 11.99165355 11.93548044\n",
            " 12.07982016 12.87533113 11.92291372 13.17748045 12.77463562 12.29742037\n",
            " 12.44945001 12.27423314 12.44938301 12.68738612 11.48744293 12.09294686\n",
            " 12.70377324 12.45477652 11.86470087 13.59076876 11.23433862 12.13775744\n",
            " 12.70918345 12.34530688 12.67303461 12.40944139 12.28433595 12.36128948\n",
            " 12.7008786  12.57507179 12.11883138 12.44859275 12.29201016 11.82194505\n",
            " 11.91682041 11.55244151 13.0432612  12.01568152 12.38104273 11.77753274\n",
            " 11.91986181 11.77914657 12.32409993 13.20140162 12.67190941 12.99665869\n",
            " 12.1142171  12.38934131 12.44830013 11.85130823 11.63845966 12.33872321\n",
            " 12.41834094 12.57562684 12.61389742 12.58307333 11.98606666 12.7886006\n",
            " 12.44566667 12.83665563 12.52416462 12.69435424 11.85208542 13.20045678\n",
            " 12.68795836 12.34363598 12.83841044 11.86530694 12.13015281 12.47789415\n",
            " 12.79648274 11.58776112 12.34530688 12.70377324 12.7097129  11.80903595\n",
            " 12.12525283 12.39158405 11.93548044 11.91159991 12.77463562 13.27749607\n",
            " 11.90835048 12.42838125 12.4305021  12.70624967 12.42756799 12.60222681\n",
            " 12.7850141  12.74315455 12.29201016 12.44690659 12.72900137 12.33987241\n",
            " 12.29201016 11.74004955 12.66120426 11.88234549 12.22518022 11.74656619\n",
            " 13.02276793 12.48023447 11.57260802 12.19860152 12.02914855 12.12514749\n",
            " 12.52801336 11.86793119 12.37613994 11.98621908 12.60851449 11.84205569\n",
            " 12.4060907  12.65887349 12.70377324 12.86955945 12.21481438 11.93548044\n",
            " 12.39298755 12.53980087 12.52820949 12.83841044 13.44146005 11.50551893\n",
            " 11.81406502 12.74141873 12.1725461  12.63026111 12.85363402 12.71936027\n",
            " 12.70647021 12.1650898  12.42838125 12.31549503 12.72481107 11.13958157\n",
            " 12.80091372 12.42756799 11.21161901 11.91682041 11.54392968 13.4351773\n",
            " 12.29568184 12.58070977 13.67490384 12.12989976 12.26369028 12.02914855\n",
            " 13.51434894 13.30409298 12.69254338 12.73037531 12.11962384 11.82067314\n",
            " 12.34530688 12.37070547 11.11113674 12.74282441 12.37070547 12.70631401\n",
            " 13.33101271 12.2427362  11.93548044 11.89984166 12.48023447 12.2562059\n",
            " 12.96477971 11.25663245 13.06663845 12.26264044 12.1425065  11.55146603\n",
            " 12.68254816 12.13891591 12.34626734 12.4740507  12.75025303 12.48363036]\n",
            "accuracy(R2) : 0.685\n",
            "RMSE:  0.4403006561339874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#그래디언트 부스트(Gradient Boost)"
      ],
      "metadata": {
        "id": "wDgpjhx0ApEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1) 분류예측"
      ],
      "metadata": {
        "id": "TaAZgGMHA0f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 분류선택\n",
        "X = df[[\"총매출액\", \"구매금액대\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\"]]\n",
        "Y = df[\"할인민감여부\"]\n",
        "\n",
        "#2. train-test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#3. 데이터 전처리(preprocessing)\n",
        "ct = ColumnTransformer([(\"scaling\", StandardScaler(), [\"총매출액\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\"]), \n",
        "                        (\"onehot\", OneHotEncoder(sparse=False), [\"구매금액대\"])])\n",
        "ct.fit(X_train)\n",
        "X_train = ct.transform(X_train)\n",
        "X_test = ct.transform(X_test)\n",
        "\n",
        "#4. 오버샘플링\n",
        "smote = SMOTE(random_state = 0)\n",
        "X_train, Y_train = smote.fit_resample(X_train, Y_train)"
      ],
      "metadata": {
        "id": "CuE_IbOAAolH"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. 모형생성\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "model = GradientBoostingClassifier(random_state = 0, n_estimators = 100, max_depth = 4, \n",
        "                                   learning_rate = 0.1) \n",
        "#learning_rate: 전 모형으로 부터 나온 오차를 다음 모델로 넘어갈때 얼마나 배울것인가\n",
        "\n",
        "#6. 모형 학습 예측\n",
        "model.fit(X_train, Y_train)\n",
        "Y_pred = model.predict(X_test)\n",
        "print(\"Y predict value : \\n\", Y_pred)\n",
        "print(\"accuracy(test) : {:.3f}\".format(model.score(X_test, Y_test)))\n",
        "print(classification_report(Y_test, Y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-REGNWL1A8OJ",
        "outputId": "da2da232-e2d8-4c0a-fafe-cf21c5adf612"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value : \n",
            " [1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1\n",
            " 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1\n",
            " 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 0 0]\n",
            "accuracy(test) : 0.933\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.97      0.94       177\n",
            "           1       0.95      0.89      0.92       123\n",
            "\n",
            "    accuracy                           0.93       300\n",
            "   macro avg       0.94      0.93      0.93       300\n",
            "weighted avg       0.93      0.93      0.93       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2)회귀예측"
      ],
      "metadata": {
        "id": "77U__TXnCC3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 변수선택\n",
        "X = df[df.이탈여부 == 0][[\"방문빈도\", \"총 할인 금액\", \"고객등급\", \"구매유형\", \"거래기간\", \"할인민감여부\", \"평균 구매주기\"]]\n",
        "Y = np.log1p(df[df.이탈여부 == 0][\"1회 평균매출액\"])\n",
        "\n",
        "#2. train-test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "#3. 데이터 전처리(preprocessing)\n",
        "ct = ColumnTransformer([(\"scaling\", StandardScaler(), [\"방문빈도\", \"총 할인 금액\", \"거래기간\", \"평균 구매주기\"]), \n",
        "                        (\"onehot\", OneHotEncoder(sparse = False), [\"고객등급\", \"구매유형\", \"할인민감여부\"])])\n",
        "ct.fit(X_train)\n",
        "X_train = ct.transform(X_train)\n",
        "X_test = ct.transform(X_test)"
      ],
      "metadata": {
        "id": "Y_19Qc50CCcJ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. 모형생성\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "model = GradientBoostingRegressor(random_state = 0, n_estimators = 100, max_depth = 4, \n",
        "                                   learning_rate = 0.1)\n",
        "\n",
        "#5. 모형 학습 예측\n",
        "model.fit(X_train, Y_train)\n",
        "Y_pred = model.predict(X_test)\n",
        "print(\"Y predict value : \\n\", Y_pred)\n",
        "print(\"accuracy(R2) : {:.3f}\".format(model.score(X_train, Y_train)))\n",
        "rmse = sqrt(mean_squared_error(Y_test, Y_pred))\n",
        "print(\"RMSE: \", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLhsjmHFCn1f",
        "outputId": "6b72e971-7987-4ad9-fb73-db9585673bae"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value : \n",
            " [12.62344813 12.25583406 12.09458751 13.07090617 11.40422303 12.45220348\n",
            " 11.61198904 12.45651119 12.08328814 12.32640447 12.39533751 12.23594472\n",
            " 12.13577357 12.24057416 12.73503493 12.29364736 12.84361351 12.99742565\n",
            " 11.76512764 12.45752565 12.63483127 11.38579233 12.41560355 12.50712961\n",
            " 10.79435548 12.62928969 12.60613938 11.92964399 12.34916462 12.07520652\n",
            " 12.57121312 13.01697524 12.39965923 13.03165827 12.11711146 12.09888134\n",
            " 12.18727902 12.94489209 11.93717047 13.18376863 12.76957258 12.21597158\n",
            " 12.54049819 12.10678757 12.64544285 12.59000213 11.18198188 12.13358256\n",
            " 12.5668075  12.42634623 11.81913361 13.52008205 11.10458182 12.09979475\n",
            " 13.07232151 12.20069308 12.66489797 12.28476526 11.98447614 12.31654311\n",
            " 12.79736561 12.76535462 11.92863698 12.44145179 12.12157567 12.10310068\n",
            " 11.87519738 11.62364672 12.99672955 12.27936785 12.36258046 11.70831635\n",
            " 11.91861812 11.96233767 12.15305515 13.12437922 12.65319979 12.93053239\n",
            " 12.03260309 12.25751434 12.0000299  11.89763423 11.5329378  12.93737778\n",
            " 12.40440044 12.64050116 12.73633832 12.62223461 11.7388871  12.5605956\n",
            " 12.46170754 13.27233418 12.49065444 12.99839492 11.98770272 13.31580316\n",
            " 13.01839043 12.06375631 12.80479374 11.98398262 11.65401981 12.2787555\n",
            " 12.73424086 11.43216014 12.24460123 12.8969792  12.85568613 11.95915872\n",
            " 12.04694305 12.60039447 12.01332333 11.79424302 12.69414998 13.32348351\n",
            " 11.70900777 12.09118735 12.18245925 12.88276381 12.17369179 12.50951424\n",
            " 12.72249627 12.60660354 12.17592955 12.74111678 12.65434081 12.24543644\n",
            " 12.44711341 11.5965925  12.66089249 11.8160437  12.76149045 11.7436314\n",
            " 13.03109978 12.47939971 11.51054282 12.14448715 11.47094125 12.11272934\n",
            " 12.91853515 11.72347747 12.41600997 12.02868338 12.60354515 11.74879952\n",
            " 12.5937395  12.63427503 12.89215104 12.73329149 12.58830131 11.85372773\n",
            " 12.44993728 12.89737544 12.4613579  12.89249471 13.5522812  11.48000074\n",
            " 11.50736548 12.67931101 12.15982715 12.60083252 13.03940439 12.73213993\n",
            " 12.75970949 11.94740293 12.66238056 12.38212172 12.7089022  11.21000233\n",
            " 12.72847666 12.15336244 11.21970406 11.8446597  11.87301358 13.47804771\n",
            " 12.35228515 12.60249735 13.83729506 11.86136594 12.63613548 11.25707904\n",
            " 13.54480221 13.48288458 12.66406609 12.72222242 12.05683865 11.94755448\n",
            " 12.37945452 12.37595316 10.65869553 13.10374396 12.37640418 12.58991922\n",
            " 13.19704136 12.59201926 11.85361937 12.02339775 12.40938986 11.80883328\n",
            " 13.01162095 11.53925824 13.19678706 12.58870571 12.0776148  11.41688057\n",
            " 12.70617545 11.84467509 12.43060262 12.42450198 12.92575457 12.54130517]\n",
            "accuracy(R2) : 0.916\n",
            "RMSE:  0.41380793566137386\n"
          ]
        }
      ]
    }
  ]
}